{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled26.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "l72IuzKlTGcQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Классификация\n",
        "\n",
        "В прошлый раз мы рассмотрели предсказание вероятностей для 2 классов.\n",
        "\n",
        "Что делать в случае нескольких классов?\n",
        "\n",
        "###Использовать несколько ,бинарных классификаторов\n",
        "\n",
        "    Для каждого класса обучим классификатор, который делит объекты на заданный класс и все остальные. Напрмер для первого классификатора разделим вск объекты на 2 класса: первый и все остальные. Далее обучим его как в прошлый раз. Аналогично обучим остальные классификаторы для других классов.\n",
        "    Для предсказания класса нового объекта будем искать максимум из предсказанных вероятностей среди всех классификаторов. \n",
        "    \n",
        "    \n",
        "###Cпециальную функцию, которя называется softmax.\n",
        "\n",
        "Пусть  у нас 3 класса, нам надо предсказать вероятности для каждого из них.  Вероятности должны лежатьв диапазоне [0, 1] и их сумма должна быть равна 1.\n",
        "\n",
        "Введем набор весов для каждого класса: для класса 1 введем ветор $\\vec{\\omega_1}$, для класса 2 - $\\vec{\\omega_2}$ и $\\vec{\\omega_3}$ для класса 3. Эти 3 вектора можно обединить в матрицу параметров:\n",
        "$$W=(\\vec{\\omega_1},\\;\\vec{\\omega_2},\\;\\vec{\\omega_3})$$\n",
        "\n",
        "\n",
        "Мы можем посчитать для каждого примера три выражения: $\\vec{x}\\cdot\\vec{\\omega_1}$, $\\vec{x}\\cdot\\vec{\\omega_2}$ и Тогда мы можем посчитать для каждого примера три выражения: $\\vec{x}\\cdot\\vec{\\omega_1}$,  $\\vec{x}\\cdot\\vec{\\omega_2}$ и $\\vec{x}\\cdot\\vec{\\omega_3}$.\n",
        "\n",
        "В матричной форме:\n",
        "$$margin = W\\cdot\\vec{x}$$\n",
        "Получим вектор из 3 компонент.\n",
        "\n",
        "Однако эти выражения могут принимать значения в диапазоне $(-\\infty, \\infty)$. Преобразуем их в верояности:\n",
        "$$p_1=\\frac{e^{\\vec{x}\\cdot\\vec{\\omega_1}}}{e^{\\vec{x}\\cdot\\vec{\\omega_1}}+e^{\\vec{x}\\cdot\\vec{\\omega_2}}+e^{\\vec{x}\\cdot\\vec{\\omega_3}}}$$\n",
        "$$p_2=\\frac{e^{\\vec{x}\\cdot\\vec{\\omega_2}}}{e^{\\vec{x}\\cdot\\vec{\\omega_1}}+e^{\\vec{x}\\cdot\\vec{\\omega_2}}+e^{\\vec{x}\\cdot\\vec{\\omega_3}}}$$\n",
        "$$p_3=\\frac{e^{\\vec{x}\\cdot\\vec{\\omega_3}}}{e^{\\vec{x}\\cdot\\vec{\\omega_1}}+e^{\\vec{x}\\cdot\\vec{\\omega_2}}+e^{\\vec{x}\\cdot\\vec{\\omega_3}}}$$\n",
        "\n",
        "Можно убедиться, что такие выражения положительны и в сумме дают 1.\n",
        "Эти три вероятности обединяют в вектор $\\vec{p}$:\n",
        "$$\\vec{p}=(p_1,\\;p_2,\\;p_3)$$\n",
        "\n",
        "\n",
        "\n",
        "Для обучения модели представим метки классов в виде векторов, для первого класса:\n",
        "$$\\vec{y_1}=(1,\\;0,\\;0)$$\n",
        "$$\\vec{y_2}=(0,\\;1,\\;0)$$\n",
        "$$\\vec{y_3}=(0,\\;0,\\;1)$$\n",
        "\n",
        "\n",
        "В качестве функции потерь мы можем использовать стандартную среднюю разность квадратов. Для каждого примера мы знаем корректный класс, преобразуем его в соответствующий вектор и обозначи $\\vec{y_i}$. Тогда можем посчитать отклонение каждой компоненты:\n",
        "$$L=\\frac{1}{N}\\sum_i ((p_{i1}-y_{i1})^2+(p_{i2}-y_{i2})^2+(p_{i3}-y_{i3})^2)$$\n",
        "\n",
        "На практике используют другую функцию потерь, которая дает лучшие результаты при обучении: «логлосс» (logloss / log_loss), перекрёстной / кросс-энтропией (Cross Entropy).\n",
        "\n",
        "Для одного объекта:\n",
        "$$L_i  = \\sum_j -y_ij\\cdot log(p_{ij})$$\n",
        "В случае трех классов:\n",
        "$$L_i  = -y_{i1}\\cdot log(p_{i1})-y_{i2}\\cdot log(p_{i2})-y_{i3}\\cdot log(p_{i3})$$\n",
        "Полная функция потерь:\n",
        "$$L=\\frac{1}{N}\\sum_i L_i$$"
      ]
    },
    {
      "metadata": {
        "id": "9NJS27CoTEux",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# посчитать кросс энтропию для заданных данных и весов\n",
        "import numpy as np\n",
        "\n",
        "n =100\n",
        "# генерируем данные\n",
        "max_range = 10\n",
        "X = (np.random.random((n,4))-0.5) * max_range \n",
        "y = np.random.randint(0,3,(n))\n",
        "\n",
        "# генерируем данные\n",
        "W = np.random.random((4,3))\n",
        "\n",
        "scalar = None\n",
        "\n",
        "exps = None\n",
        "\n",
        "probabilities = None\n",
        "\n",
        "# преобразуем метки в ветора\n",
        "y_vec = np.zeros((n,3))\n",
        "\n",
        "# logloss\n",
        "L = None"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}